{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "professional-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sorted-clinic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "worthy-tract",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "material-jacob",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  Box(-inf, inf, (8,), float32)\n",
      "Number of Actions:  Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(5)\n",
    "print('State shape: ', env.observation_space)\n",
    "print('Number of Actions: ', env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eleven-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to check if env is working correctly \n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lesbian-dress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-100], device='cuda:0', dtype=torch.int32)\n",
      "tensor([-9.2983e-02, -1.4844e-03, -1.0857e-02,  5.6890e-02, -9.9618e-04,\n",
      "         1.8996e-01,  1.0000e+00,  1.0000e+00], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "next_state, reward, done, data = env.step(2)\n",
    "reward\n",
    "reward = torch.from_numpy(np.array([reward])).to(device)\n",
    "print(reward)\n",
    "next_state = torch.from_numpy(next_state).to(device)\n",
    "print(next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "outside-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self,seed):\n",
    "        super(Network, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(8, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 4)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        h = F.relu(self.fc1(state))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        y = self.fc3(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "imperial-america",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cuda() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-eeaa45bc76b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mNetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: cuda() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "infrared-steel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.6900e-04,  1.4214e+00,  6.7750e-02,  4.6374e-01, -7.6845e-04,\n",
       "        -1.5346e-02,  0.0000e+00,  0.0000e+00])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.from_numpy(env.reset())\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "gentle-combat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0775, -0.1121,  0.0074, -0.0513], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Network(5)\n",
    "m.cuda()\n",
    "c = m(next_state)\n",
    "c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "administrative-millennium",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(object):\n",
    "    def __init__(self,alpha,gamma,epsilon,n_eps,N,C,M,seed):\n",
    "        self.memory = []\n",
    "        self.memory_max = N\n",
    "        self.target_update = C\n",
    "        self.Q_t = Network(seed).to(device)\n",
    "        self.Q = Network(seed).to(device)\n",
    "        self.alpha = alpha\n",
    "        self.optimizer = optim.SGD(self.Q.parameters(), lr=self.alpha)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.seed = seed\n",
    "        self.C = C\n",
    "        self.n_eps = n_eps\n",
    "        self.mini_batch_size = M\n",
    "        self.env = gym.make('LunarLander-v2')\n",
    "        env.seed(seed)\n",
    "    \n",
    "    def store_memory(state,action,reward,next_state,done = 0):\n",
    "        reward = np.array([reward],dtype = float)\n",
    "        action = np.array([action],dtype = int)\n",
    "        done = np.array([done],dtype = int)\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "    \n",
    "    def sample_memory(M):\n",
    "        batch = np.array(random.sample(self.memory, k=M),dtype = object)\n",
    "        batch = batch.T\n",
    "        batch = batch.tolist()\n",
    "        return (torch.tensor(batch[0]).to(device),torch.tensor(batch[1]).to(device),torch.tensor(batch[2],dtype = torch.float).to(device),torch.tensor(batch[3]).to(device),torch.tensor(batch[4]).to(device))\n",
    "    \n",
    "    def solve(self):\n",
    "        states = self.env.observation_space\n",
    "        actions = self.env.action_space\n",
    "        np.random.seed(self.seed)\n",
    "        count = 0\n",
    "        scores = []\n",
    "        for eps in range(self.n_eps):\n",
    "            state = torch.from_numpy(self.env.reset()).to(device)\n",
    "            score = 0\n",
    "            for i in range(1000000):\n",
    "                greed = np.random.random()\n",
    "                #Feed Forward once to predict the best action for current state\n",
    "                self.Q.eval()\n",
    "                with torch.no_grad():\n",
    "                    actions = self.Q(state)\n",
    "                self.Q.train()\n",
    "                if greed < self.epsilon:\n",
    "                    action = np.random.randint(0, 4)\n",
    "                else:\n",
    "                    action = np.argmax(actions.detach().numpy())\n",
    "                next_state, reward, done, data = self.env.step(action)\n",
    "                score+=reward\n",
    "                self.store_memory(state,action,reward,next_state,done)\n",
    "                \n",
    "                if len(self.memory)<self.mini_batch_size:\n",
    "                    break\n",
    "                else:\n",
    "                    transitions = self.sample_memory(self.mini_batch_size)\n",
    "                \n",
    "                state,action,reward,next_state,done = transitions\n",
    "                Q_t = self.Q_t(next_states).detach()\n",
    "                Q_tmax = Q_t.max(1)[0].unsqueeze(1)\n",
    "                case2 = rewards + self.gamma * Q_tmax\n",
    "                case1 = rewards\n",
    "#                 y_j = rewards + (gamma * Q_tmax * (1-done))  \n",
    "                y = torch.where(done<1,case2,case1)    \n",
    "                Q = self.Q(states).gather(1, actions)\n",
    "                loss = F.mse_loss(Q, y)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                    \n",
    "                if count == self.C:\n",
    "                    self.Q_t = deepcopy(self.Q)\n",
    "#                     self.Q_t.load_state_dict(self.Q.state_dict())\n",
    "                \n",
    "                state = deepcopy(next_state)\n",
    "                if done:\n",
    "                    break\n",
    "            scores.append(score)\n",
    "            print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-chapel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "solid-wallace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000002110B77D2E0>\n"
     ]
    }
   ],
   "source": [
    "# model = TargetPolicy()\n",
    "# criterion = nn.NLLLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "# print(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "acceptable-romania",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0029,  1.4095, -0.2904, -0.0634,  0.0033,  0.0658,  0.0000,  0.0000])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = torch.from_numpy(env.reset())\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-privacy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
